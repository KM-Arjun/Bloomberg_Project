{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2phpjPQlL-7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf9e85a-fb87-4d45-eef1-4af5aa5f13a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.4.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.4.0)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Installing collected packages: torchmetrics, tensorboardX, lightning-utilities, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.4.2 pytorch-lightning-1.8.4.post0 tensorboardX-2.5.1 torchmetrics-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install pytorch_lightning\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from IPython import display\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch import distributions as D\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHVyInyTv5Mp"
      },
      "outputs": [],
      "source": [
        "def removeInf(df):\n",
        "    for col in df.columns[3:]:\n",
        "      df[\"test\"] = np.isinf(np.array(df.loc[:, col]))\n",
        "      df = df.loc[df[\"test\"] != True]\n",
        "      df = df.drop(columns = \"test\")\n",
        "    return df\n",
        "\n",
        "def removeNaN(df):\n",
        "    return df.dropna()\n",
        "\n",
        "def getLogTransData(data, x):\n",
        "  data[x] = np.log(data[x])\n",
        "  return data\n",
        "\n",
        "def getOutlierFree(df, conf, y):\n",
        "  # conf = 0.25\n",
        "  left = df[y].quantile(q=conf/100)\n",
        "  right = df[y].quantile(q=1-conf/100)\n",
        "  return df.loc[(df[y] > left) & (df[y] < right)].dropna()\n",
        "\n",
        "def getEncoded(df, col):\n",
        "\n",
        "  #creating instance of one-hot-encoder\n",
        "  encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "  #perform one-hot encoding on 'team' column\n",
        "  encoder_df = pd.DataFrame(encoder.fit_transform(df[[col]]).toarray())\n",
        "  encoder_df.columns = np.sort(df[col].unique())\n",
        "  \n",
        "  col = encoder_df.columns\n",
        "  df = df.reset_index(drop = True)\n",
        "  #merge one-hot encoded columns back with original DataFrame\n",
        "  return df.join(encoder_df), col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jilc7DjZp21Y"
      },
      "outputs": [],
      "source": [
        "class dataProcess():\n",
        "  def __init__(self, loc, fileName, predWindow, y):\n",
        "      self.location = loc\n",
        "      self.fileName = fileName\n",
        "      self.predWindow = predWindow\n",
        "      self.y = y\n",
        "\n",
        "  def downloadData(self):\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    link = self.location\n",
        "   # to get the id part of the file\n",
        "    id = link.split(\"/\")[-2]\n",
        "    downloaded = drive.CreateFile({'id':id})\n",
        "    downloaded.GetContentFile(fileName)\n",
        "    transDF = pd.read_csv(fileName)\n",
        "    return transDF\n",
        "  \n",
        "  def processData(self):\n",
        "    transDF = self.downloadData()\n",
        "    transDF = transDF.iloc[:, 1:].loc[(transDF[\"Emission\"]>=0) & (transDF[\"Revenue\"]>0)]\n",
        "    transDF[\"E2R\"] = 100*(transDF[\"Emission\"] / transDF[\"Revenue\"])\n",
        "    transDF[\"Net Income\"] = 100*(transDF[\"Net Income\"] / transDF[\"Revenue\"])\n",
        "    transDF[\"Market Cap\"] = 100*(transDF[\"Market Cap\"] / transDF[\"Revenue\"])\n",
        "    transDF[\"Cash Flow\"] = 100*(transDF[\"Cash Flow\"] / transDF[\"Revenue\"])\n",
        "    transDF = transDF.drop(columns = [\"Revenue\", \"Emission\"], axis = 1)\n",
        "    transDF = removeInf(getLogTransData(transDF, \"Market Cap\")).dropna()\n",
        "    return transDF\n",
        "  \n",
        "  ##new algo for getting level data\n",
        "  def getLevelData(self):\n",
        "    df = self.processData()\n",
        "    df = df.sort_values(by=['Company', 'Year'])\n",
        "    if self.predWindow == 0:\n",
        "      return df.dropna()\n",
        "    else:\n",
        "      df[self.y + \" Shift\"] = df[self.y].shift(-self.predWindow)\n",
        "\n",
        "      ## Dropping the data in the prediction window\n",
        "      erase = list(np.sort(df[\"Year\"].unique()))[-1-self.predWindow:-1][0]\n",
        "      df = df.loc[df[\"Year\"] <= erase]\n",
        "    return getOutlierFree(df, 0.25, y)\n",
        "\n",
        "class trainTestSplit(dataProcess):\n",
        "  def __init__(self, model, yNew, X, catVar, loc, fileName, predWindow, y):\n",
        "      super(trainTestSplit, self).__init__(loc, fileName, predWindow, y)\n",
        "      self.model = model\n",
        "      self.yNew = yNew\n",
        "      self.X = X\n",
        "      self.catVar = catVar\n",
        "  \n",
        "  def getTrainTestData(self):\n",
        "    df = self.getLevelData()\n",
        "    df = df.loc[df[self.yNew] != 0]\n",
        "    \n",
        "    if self.model == \"NN\":\n",
        "      dataDict = {}\n",
        "      dataDict[\"XTrainCont\"] = df[self.X].loc[df[\"Year\"] < max(df[\"Year\"])].dropna().reset_index(drop = True)\n",
        "      dataDict[\"XTestCont\"] = df[self.X].loc[df[\"Year\"] == max(df[\"Year\"])].dropna().reset_index(drop = True)\n",
        "      dataDict[\"XTrainCat\"] = df[self.catVar].loc[df[\"Year\"] < max(df[\"Year\"])].dropna()\n",
        "      dataDict[\"XTestCat\"] = df[self.catVar].loc[df[\"Year\"] == max(df[\"Year\"])].dropna()\n",
        "      dataDict[\"yTrain\"] = np.log(df[self.yNew].loc[df[\"Year\"] < max(df[\"Year\"])].dropna())\n",
        "      dataDict[\"yTest\"] = np.log(df[self.yNew].loc[df[\"Year\"] == max(df[\"Year\"])].dropna())\n",
        "      return dataDict\n",
        "    \n",
        "    elif self.model == \"RF\":\n",
        "      df, newCol = getEncoded(df, self.catVar)[0], getEncoded(df, self.catVar)[1] \n",
        "      dataDict = {}\n",
        "      XVar = self.X + list(newCol)\n",
        "      dataDict[\"XTrain\"] = df[XVar].loc[df[\"Year\"] < max(df[\"Year\"])].dropna().reset_index(drop = True)\n",
        "      dataDict[\"XTest\"] = df[XVar].loc[df[\"Year\"] == max(df[\"Year\"])].dropna().reset_index(drop = True)\n",
        "      dataDict[\"yTrain\"] = np.log(df[self.yNew].loc[df[\"Year\"] < max(df[\"Year\"])].dropna())\n",
        "      dataDict[\"yTest\"] = np.log(df[self.yNew].loc[df[\"Year\"] == max(df[\"Year\"])].dropna())\n",
        "      return dataDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfV71WC8RK0"
      },
      "outputs": [],
      "source": [
        "class RFReg():\n",
        "  def __init__(self, nEst, criterion, XTrain, yTrain, XTest, yTest):\n",
        "      self.nEst = nEst\n",
        "      self.criterion = criterion\n",
        "      self.XTrain, self.XTest = XTrain, XTest\n",
        "      self.yTrain, self.yTest = yTrain, yTest\n",
        "  \n",
        "  def trainRF(self):\n",
        "    # criterion{“squared_error”, “absolute_error”, “poisson”}\n",
        "    # create regressor object\n",
        "\n",
        "    self.regressor1 = RandomForestRegressor(n_estimators = self.nEst, random_state = 0, criterion=self.criterion)\n",
        "    model = self.regressor1.fit(self.XTrain, self.yTrain)\n",
        "    yFit = model.predict(self.XTrain)\n",
        "    variance = np.square(yFit - self.yTrain)\n",
        "    self.regressor2 = RandomForestRegressor(n_estimators = self.nEst, random_state = 42, criterion=self.criterion)\n",
        "    modelVar = self.regressor2.fit(self.XTrain, variance)\n",
        "    return model, modelVar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOoacAuGfaYH"
      },
      "outputs": [],
      "source": [
        "class embedCategory():\n",
        "  def __init__(self, trainX, layerCnt):\n",
        "      self.trainX = trainX\n",
        "      self.layerCnt = layerCnt\n",
        "\n",
        "  def getIndDict(self):\n",
        "    industryCategoryDict = {}\n",
        "    j = 0\n",
        "    for ind in np.unique(self.trainX):\n",
        "      industryCategoryDict[ind] = j\n",
        "      j = j + 1\n",
        "    return industryCategoryDict\n",
        "\n",
        "  def embedLayer(self):\n",
        "    indDict = self.getIndDict()\n",
        "    embed_layers = nn.ModuleDict({\n",
        "            \"Industry\": nn.Embedding(max(indDict.values()) + 1, self.layerCnt)\n",
        "        })\n",
        "    return embed_layers\n",
        "\n",
        "class ConditionalNet(torch.nn.Module):\n",
        "    def __init__(self, lenX, layerCounts, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lenX = lenX\n",
        "        self.layerCounts = layerCounts\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.lenX+self.layerCounts, hidden_size),# we increased this to account for the embedding output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class ConditionalModel(pl.LightningModule):\n",
        "    def __init__(self, condNet, embedLayers, margin):\n",
        "        super().__init__()\n",
        "        self.condNet = condNet\n",
        "        self.embed_layers = embedLayers\n",
        "        self.margin = margin\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, categorical_map, y = batch\n",
        "        #print('here in training step with',x.shape,y.shape,categorical_map)\n",
        "        cond_emb = torch.stack([self.embed_layers[k](v) for k, v in categorical_map.items()],-1).sum(-1).squeeze(-2)\n",
        "        # print(cond_emb.shape)\n",
        "        # print(x.shape)\n",
        "        full_cond = torch.concat((x.squeeze(),cond_emb),-1)\n",
        "        # print(full_cond.shape)\n",
        "        dist_params = self.condNet(full_cond)\n",
        "        dist_params = dist_params.chunk(2, -1)\n",
        "        _mus = dist_params[0]\n",
        "        _stds = torch.nn.Softplus()(dist_params[1]) + self.margin\n",
        "        return -D.Normal(_mus, _stds).log_prob(y).mean()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001) ## hyperparameterize \n",
        "        return optimizer\n",
        "\n",
        "class CondDataset(Dataset):\n",
        "    def __init__(self, e, industry, indDict, y):\n",
        "        self.e = e\n",
        "        self.ind = industry\n",
        "        self.indDict = indDict\n",
        "        self.y = y\n",
        "        return\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print('here with idx',idx,'and vals',self.e[idx],self.y[idx],self.color[idx],self.brand[idx])\n",
        "        categorical_map = {\n",
        "            'Industry':torch.Tensor(np.array([self.indDict[self.ind[idx]]])).long().squeeze()\n",
        "        }\n",
        "        return (torch.Tensor(np.array([self.e[idx]])), categorical_map, torch.Tensor(np.array([self.y[idx]])))\n",
        "        \n",
        "    def __len__(self):  \n",
        "        return len(self.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sym8dzJ-fO-8"
      },
      "outputs": [],
      "source": [
        "def dataLoad(contVar, catVar, catDict, y):\n",
        "  dataloader = DataLoader(\n",
        "    CondDataset(contVar, catVar, catDict, y),\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "  )\n",
        "  return dataloader\n",
        "\n",
        "def trainModel(lenX, layerCounts, hidden_size, embed_layers, margin, data):\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=1500,\n",
        "      accelerator=\"cpu\",\n",
        "      num_sanity_val_steps=0\n",
        "  )\n",
        "  model = ConditionalModel(ConditionalNet(lenX, layerCounts, hidden_size), embed_layers, margin)\n",
        "  trainer.fit(model, data)\n",
        "  model.eval()\n",
        "  return model.state_dict() #trainer.fit(model, data)\n",
        "\n",
        "def NLLCalc():\n",
        "  losses = []\n",
        "  cdfs = []\n",
        "  CatXTest = np.array(trainTestDict[\"XTestCat\"].dropna().reset_index(drop = True))\n",
        "  ContXTest = np.array(trainTestDict[\"XTestCont\"].dropna().reset_index(drop = True))\n",
        "  indDictTest = embedCategory(CatXTest, layerCounts).getIndDict()\n",
        "  yTest = np.array(trainTestDict[\"yTest\"].dropna().reset_index(drop = True))\n",
        "  embed_layers = embedCategory(CatXTest, layerCounts).embedLayer()\n",
        "  model = ConditionalModel(ConditionalNet(lenX, layerCounts, hidden_size), embed_layers, margin)\n",
        "  with torch.no_grad():\n",
        "      dataloader2 = DataLoader(\n",
        "          CondDataset(ContXTest, CatXTest, indDictTest, yTest),\n",
        "          batch_size=256,\n",
        "          shuffle=False,\n",
        "          drop_last=False,\n",
        "      )\n",
        "      for batch in dataloader2:\n",
        "          x, categorical_map, y = batch\n",
        "          cond_emb = torch.stack([model.embed_layers[k](v) for k, v in categorical_map.items()],-1).sum(-1).squeeze(-2)\n",
        "          #print(cond_emb.shape)\n",
        "          full_cond = torch.concat((x.squeeze(),cond_emb),-1)\n",
        "          #print(full_cond.shape)\n",
        "          dist_params = model.condNet(full_cond)\n",
        "          dist_params = dist_params.chunk(2, -1)\n",
        "          _mus = dist_params[0]\n",
        "          _stds = torch.nn.Softplus()(dist_params[1]) + margin\n",
        "          dists = D.Normal(_mus, _stds)\n",
        "          losses.extend(list(dists.log_prob(y).cpu().detach().numpy()))\n",
        "          cdfs.extend(list(dists.cdf(y).cpu().detach().numpy()))\n",
        "  losses = np.array(losses)\n",
        "  cdfs = np.array(cdfs)\n",
        "  return np.mean(-losses), cdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVYfTiyU5Z6C"
      },
      "outputs": [],
      "source": [
        "## Initialization\n",
        "loc = 'https://drive.google.com/file/d/1TQodZLpXVgAmaP5XZe8lYImlLaJ68yWb/view?usp=share_link'\n",
        "fileName = 'data_prep1.csv'\n",
        "predWindow = 1\n",
        "\n",
        "y = \"E2R\"\n",
        "# dataClass = dataProcess(loc, fileName, predWindow, y)\n",
        "\n",
        "yNew = \"E2R Shift\"\n",
        "X = ['Net Income','Market Cap','Cash Flow','E2R']\n",
        "catVar = \"Industry\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASoijeRi5oHc"
      },
      "outputs": [],
      "source": [
        "## Random Forrest Regressor\n",
        "trainTestClass = trainTestSplit(\"RF\", yNew, X, catVar, loc, fileName, predWindow, y)\n",
        "trainTestDict = trainTestClass.getTrainTestData()\n",
        "XTrain, yTrain, XTest, yTest = trainTestDict[\"XTrain\"], trainTestDict[\"yTrain\"], trainTestDict[\"XTest\"], trainTestDict[\"yTest\"]\n",
        "\n",
        "nEst = 100\n",
        "# absolute_error”, “friedman_mse”, “poisson”\n",
        "criterion = 'squared_error' #'squared_error'\n",
        "modelRF = RFReg(nEst, criterion, XTrain, yTrain, XTest, yTest)\n",
        "muModel, sigmaModel = modelRF.trainRF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uCtTHmvCoFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7a105a-91ae-4956-c035-e28aa4c3a97e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.943374797265357"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "muModel.score(trainTestDict[\"XTest\"], trainTestDict[\"yTest\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_njUm97eHHAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f36ba7-99c1-4a2a-c7eb-ba25b93ba0e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.013297279213491775"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "varPred = np.square(trainTestDict[\"yTest\"] - muModel.predict(trainTestDict[\"XTest\"]))\n",
        "sigmaModel.score(trainTestDict[\"XTest\"], varPred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "94e832c8d3a048db8e57c36e1568f6eb",
            "9e6559661ff24751916d74d6ff0226ce",
            "73ae0a9dcff34a4897e604823da9ab8a",
            "4bf50fc607204fd69498277808e33033",
            "38903bcc1115426a962931d3c484075c",
            "99cbb5085c8143a2baac3544e3718d29",
            "ed5e193b7808447eaa3e54b2810bd0fc",
            "ee0ba27e35af41faa35b476a0360bfca",
            "bd6de233c0c04d8a80fb2b051a70daab",
            "f5451d1e46f94a5881129ec37e057655",
            "39306fa5063348c6aff96261e3ce7492"
          ]
        },
        "id": "dowd7yk7ywc7",
        "outputId": "051739b4-853b-4cdf-8afa-5bdbc971e88f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name         | Type           | Params\n",
            "------------------------------------------------\n",
            "0 | condNet      | ConditionalNet | 19.5 K\n",
            "1 | embed_layers | ModuleDict     | 3.3 K \n",
            "------------------------------------------------\n",
            "22.8 K    Trainable params\n",
            "0         Non-trainable params\n",
            "22.8 K    Total params\n",
            "0.091     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94e832c8d3a048db8e57c36e1568f6eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Neural Net Model Execution\n",
        "trainTestClass = trainTestSplit(\"NN\", yNew, X, catVar, loc, fileName, predWindow, y)\n",
        "trainTestDict = trainTestClass.getTrainTestData()\n",
        "catVar = np.array(trainTestDict[\"XTrainCat\"].dropna().reset_index(drop = True))\n",
        "contVar = np.array(trainTestDict[\"XTrainCont\"].dropna().reset_index(drop = True))\n",
        "lenX = len(X)\n",
        "\n",
        "lc = [16]  #[16, 32, 48]\n",
        "mr = [1e-2]  #[1e-2, 1e-4, 1e-6]\n",
        "hs = [128]  #[64, 128, 256]\n",
        "\n",
        "op = pd.DataFrame(columns = [\"Layers\", \"Margin\", \"Hidden_States\", \"NLL\"])\n",
        "for l in lc:\n",
        "  for m in mr:\n",
        "    for h in hs:\n",
        "      layerCounts = l\n",
        "      catDict = embedCategory(catVar, layerCounts).getIndDict()\n",
        "      embed_layers = embedCategory(catVar, layerCounts).embedLayer()\n",
        "      \n",
        "      margin = m\n",
        "      hidden_size = h\n",
        "      y = np.array(trainTestDict[\"yTrain\"].dropna().reset_index(drop = True))\n",
        "\n",
        "      dataloader = dataLoad(contVar, catVar, catDict, y)\n",
        "      model = trainModel(lenX, layerCounts, hidden_size, embed_layers, margin, dataloader)\n",
        "      import pickle\n",
        "\n",
        "      with open('modelNN.pickle', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "      NLL = NLLCalc()[0]\n",
        "      op.loc[len(op) + 1] = [l, m, h, NLL]\n",
        "      # op.to_csv(\"modelNLL.csv\")\n",
        "      # files.download(r\"modelNLL.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zHv4cPh_cqbd"
      },
      "outputs": [],
      "source": [
        "indDict = embedCategory(catVar, layerCounts).getIndDict()\n",
        "with open('indDict.pickle', 'wb') as f:\n",
        "        pickle.dump(indDict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lIlYZgk26ofM"
      },
      "outputs": [],
      "source": [
        "## Interface code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1cVMcUAP6xAh"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "modelFile = open('modelNN.pickle', 'rb')\n",
        "stateDict = pickle.load(modelFile)\n",
        "indDictFile = open('indDict.pickle', 'rb')\n",
        "indDict = pickle.load(indDictFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqrOfuFEL8KV"
      },
      "outputs": [],
      "source": [
        "len(indDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ67hRIx65bV"
      },
      "outputs": [],
      "source": [
        "# User Input\n",
        "NetIncome = 300\n",
        "Revenue = 900\n",
        "MarketCap = 9000\n",
        "CashFlow = -350\n",
        "Emission = 70\n",
        "Industry = \"Waste Management\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS7ZHXji7YtG"
      },
      "outputs": [],
      "source": [
        "## calculate as part of the interface code\n",
        "E2R = np.log(100*(Emission/Revenue))\n",
        "NI2R = 100*(NetIncome/Revenue)\n",
        "MC2R = 100*(MarketCap/Revenue)\n",
        "CF2R = 100*(CashFlow/Revenue)\n",
        "CatX = np.array(Industry)\n",
        "ContX = np.array([NI2R, MC2R, CF2R, E2R])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arAmI-MV7Guk"
      },
      "outputs": [],
      "source": [
        "# catDict = embedCategory(indDict, 16).getIndDict()\n",
        "embed_layers = nn.ModuleDict({\n",
        "            \"Industry\": nn.Embedding(max(indDict.values()) + 1, 16)\n",
        "        })\n",
        "# embed_layers = embedCategory(indDict, 16).embedLayer()\n",
        "model = ConditionalModel(ConditionalNet(4, 16, 128), embed_layers, 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2AH8_JQLcsu"
      },
      "outputs": [],
      "source": [
        "type(embed_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdaC_AJz7J6m"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(stateDict)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7T0Mfb59ozB"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  x = torch.Tensor(ContX)\n",
        "  # 'Industry':torch.Tensor(np.array([self.indDict[self.ind[idx]]])).long().squeeze()\n",
        "  categorical_map = {\n",
        "              'Industry':torch.Tensor(np.array([indDict[Industry]])).long()#.squeeze()\n",
        "          }\n",
        "  cond_emb = torch.stack([model.embed_layers[k](v) for k, v in categorical_map.items()],-1).sum(-1).squeeze(-2)\n",
        "  #print(cond_emb.shape)\n",
        "  full_cond = torch.concat((x.squeeze(),cond_emb),-1)\n",
        "  #print(full_cond.shape)\n",
        "  dist_params = model.condNet(full_cond)\n",
        "  dist_params = dist_params.chunk(2, -1)\n",
        "  _mus = dist_params[0]\n",
        "  _stds = torch.nn.Softplus()(dist_params[1]) + margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ignx-CjPO3TJ"
      },
      "outputs": [],
      "source": [
        "_mus, _stds.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0FqpbJCPWDb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38903bcc1115426a962931d3c484075c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "39306fa5063348c6aff96261e3ce7492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bf50fc607204fd69498277808e33033": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5451d1e46f94a5881129ec37e057655",
            "placeholder": "​",
            "style": "IPY_MODEL_39306fa5063348c6aff96261e3ce7492",
            "value": " 8/248 [00:00&lt;00:20, 11.47it/s, loss=2.26e+03, v_num=0]"
          }
        },
        "73ae0a9dcff34a4897e604823da9ab8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee0ba27e35af41faa35b476a0360bfca",
            "max": 248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd6de233c0c04d8a80fb2b051a70daab",
            "value": 8
          }
        },
        "94e832c8d3a048db8e57c36e1568f6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e6559661ff24751916d74d6ff0226ce",
              "IPY_MODEL_73ae0a9dcff34a4897e604823da9ab8a",
              "IPY_MODEL_4bf50fc607204fd69498277808e33033"
            ],
            "layout": "IPY_MODEL_38903bcc1115426a962931d3c484075c"
          }
        },
        "99cbb5085c8143a2baac3544e3718d29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6559661ff24751916d74d6ff0226ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99cbb5085c8143a2baac3544e3718d29",
            "placeholder": "​",
            "style": "IPY_MODEL_ed5e193b7808447eaa3e54b2810bd0fc",
            "value": "Epoch 0:   3%"
          }
        },
        "bd6de233c0c04d8a80fb2b051a70daab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed5e193b7808447eaa3e54b2810bd0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee0ba27e35af41faa35b476a0360bfca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5451d1e46f94a5881129ec37e057655": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}